# AI Chat Widget

<div align="center">

[![npm version](https://img.shields.io/npm/v/@kendent/ai-chat-widget.svg)](https://www.npmjs.com/package/@kendent/ai-chat-widget)
[![npm downloads](https://img.shields.io/npm/dm/@kendent/ai-chat-widget.svg)](https://www.npmjs.com/package/@kendent/ai-chat-widget)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Language / è¯­è¨€**: [English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

</div>

---

<a name="english"></a>
## English

A plug-and-play, extensible, persistent, context-aware frontend conversation OS component.

### Features

- ğŸ§  **Smart Context Management** - Memory Window automatically manages conversation history with sliding window and token budget trimming
- ğŸ”Œ **Plugin Architecture** - Supports beforeSend / afterReceive / onToolCall hooks
- ğŸŒ **Multi-Model Support** - OpenAI, Claude, Moonshot, Local LLM
- âš¡ **Streaming Response** - Supports SSE / Fetch Stream with real-time token display
- ğŸ’¾ **Persistence Storage** - localStorage (recent sessions) + IndexedDB (long-term sessions)
- ğŸ¨ **Modern UI** - Clean and beautiful chat interface

### Project Structure

```
src/          # Source code
example/      # Example application
dist/         # Build output
```

- **Vite**: Unified build tool using Vite for both library and example
  - `yarn build` - Build library files
  - `yarn dev` - Run example application

### Quick Start

#### Installation

```bash
npm install @kendent/ai-chat-widget
```

#### Run Example

```bash
# After cloning the repository
yarn install
yarn dev  # Start example app, visit http://localhost:3000
```

#### Basic Usage

```tsx
import { AIChatWidget } from '@kendent/ai-chat-widget';
import '@kendent/ai-chat-widget/dist/style.css';

function App() {
  return (
    <AIChatWidget
      provider="openai"
      apiKey="sk-xxx"
      model="gpt-4"
      system="You are a helpful assistant"
      memory={50}
    />
  );
}
```

### API

#### AIChatWidget Props

| Property | Type | Required | Description |
|----------|------|----------|-------------|
| `provider` | `'openai' \| 'claude' \| 'moonshot' \| 'local'` | âœ… | Model provider |
| `apiKey` | `string` | âŒ | API key |
| `baseURL` | `string` | âŒ | Custom API endpoint |
| `model` | `string` | âœ… | Model name |
| `system` | `string` | âŒ | System prompt |
| `memory` | `number \| MemoryConfig` | âŒ | Context window configuration |
| `plugins` | `ChatPlugin[]` | âŒ | Plugin list |
| `onMessage` | `(message: Message) => void` | âŒ | Message callback |
| `onError` | `(error: Error) => void` | âŒ | Error callback |

#### MemoryConfig

```typescript
interface MemoryConfig {
  maxMessages?: number;      // Maximum message count (default: 50)
  maxTokens?: number;         // Maximum token count (default: 8000)
  enableSummarization?: boolean;  // Enable summarization compression
  systemPromptLocked?: boolean;   // Lock system prompt
}
```

### Plugin System

#### Create Plugin

```typescript
import { ChatPlugin } from '@kendent/ai-chat-widget';

const searchPlugin: ChatPlugin = {
  name: 'search',
  beforeSend: async (ctx) => {
    // Modify messages before sending
    return ctx;
  },
  afterReceive: async (ctx) => {
    // Process response after receiving
    return ctx.response;
  },
  onToolCall: async (tool, ctx) => {
    // Handle tool calls
    return result;
  },
};
```

#### Use Plugin

```tsx
<AIChatWidget
  provider="openai"
  apiKey="sk-xxx"
  model="gpt-4"
  plugins={[searchPlugin]}
/>
```

### Advanced Usage

#### Custom Provider

```typescript
import { BaseProvider, ProviderHub } from '@kendent/ai-chat-widget';

class CustomProvider extends BaseProvider {
  name = 'custom';
  
  async *stream(messages, config, signal) {
    // Implement streaming interface
  }
  
  async chat(messages, config, signal) {
    // Implement chat interface
  }
}

const hub = new ProviderHub();
hub.register(new CustomProvider());
```

#### Direct Usage of Orchestrator

```typescript
import { ConversationOrchestrator } from '@kendent/ai-chat-widget';

const orchestrator = new ConversationOrchestrator({
  provider: {
    provider: 'openai',
    apiKey: 'sk-xxx',
    model: 'gpt-4',
  },
  memory: 50,
  systemPrompt: 'You are a helpful assistant',
});

const response = await orchestrator.sendMessage('Hello!', {
  onChunk: (chunk) => console.log(chunk),
  onComplete: (message) => console.log(message),
});
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UI Layer    â”‚
â”‚ ChatWindow UI â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation  â”‚  â† Core Engine
â”‚ Orchestrator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Memory â”‚ â”‚Prompt â”‚ â”‚Providerâ”‚
â”‚Window â”‚ â”‚Engine â”‚ â”‚  Hub  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Supported Models

- **OpenAI**: GPT-4, GPT-3.5-turbo, etc.
- **Claude**: Claude 3 Opus, Sonnet, Haiku
- **Moonshot**: Moonshot-v1-8k, Moonshot-v1-32k
- **Local**: Ollama, LM Studio, etc. (OpenAI-compatible)

### License

MIT

---

<a name="ä¸­æ–‡"></a>
## ä¸­æ–‡

ä¸€ä¸ªå³æ’å³ç”¨ã€å¯æ‰©å±•ã€å¯æŒä¹…åŒ–ã€æœ‰ä¸Šä¸‹æ–‡æ™ºèƒ½çš„å‰ç«¯å¯¹è¯æ“ä½œç³»ç»Ÿç»„ä»¶ã€‚

### ç‰¹æ€§

- ğŸ§  **æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†** - Memory Window è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²ï¼Œæ”¯æŒæ»‘åŠ¨çª—å£å’Œ Token é¢„ç®—è£å‰ª
- ğŸ”Œ **æ’ä»¶åŒ–æ¶æ„** - æ”¯æŒ beforeSend / afterReceive / onToolCall é’©å­
- ğŸŒ **å¤šæ¨¡å‹æ”¯æŒ** - OpenAI, Claude, Moonshot, Local LLM
- âš¡ **æµå¼å“åº”** - æ”¯æŒ SSE / Fetch Streamï¼Œå®æ—¶æ˜¾ç¤º token
- ğŸ’¾ **æŒä¹…åŒ–å­˜å‚¨** - localStorage (æœ€è¿‘ä¼šè¯) + IndexedDB (é•¿æœŸä¼šè¯)
- ğŸ¨ **ç°ä»£åŒ– UI** - ç®€æ´ç¾è§‚çš„èŠå¤©ç•Œé¢

### é¡¹ç›®ç»“æ„

```
src/          # æºä»£ç 
example/      # ç¤ºä¾‹åº”ç”¨
dist/         # æ„å»ºè¾“å‡º
```

- **Vite**: ç»Ÿä¸€ä½¿ç”¨ Vite æ„å»ºåº“å’Œè¿è¡Œç¤ºä¾‹
  - `yarn build` - æ„å»ºåº“æ–‡ä»¶
  - `yarn dev` - è¿è¡Œç¤ºä¾‹åº”ç”¨

### å¿«é€Ÿå¼€å§‹

#### å®‰è£…

```bash
npm install @kendent/ai-chat-widget
```

#### è¿è¡Œç¤ºä¾‹

```bash
# å…‹éš†é¡¹ç›®å
yarn install
yarn dev  # å¯åŠ¨ç¤ºä¾‹åº”ç”¨ï¼Œè®¿é—® http://localhost:3000
```

#### åŸºç¡€ç”¨æ³•

```tsx
import { AIChatWidget } from '@kendent/ai-chat-widget';
import '@kendent/ai-chat-widget/dist/style.css';

function App() {
  return (
    <AIChatWidget
      provider="openai"
      apiKey="sk-xxx"
      model="gpt-4"
      system="You are a helpful assistant"
      memory={50}
    />
  );
}
```

### API

#### AIChatWidget Props

| å±æ€§ | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| `provider` | `'openai' \| 'claude' \| 'moonshot' \| 'local'` | âœ… | æ¨¡å‹æä¾›å•† |
| `apiKey` | `string` | âŒ | API å¯†é’¥ |
| `baseURL` | `string` | âŒ | è‡ªå®šä¹‰ API åœ°å€ |
| `model` | `string` | âœ… | æ¨¡å‹åç§° |
| `system` | `string` | âŒ | ç³»ç»Ÿæç¤ºè¯ |
| `memory` | `number \| MemoryConfig` | âŒ | ä¸Šä¸‹æ–‡çª—å£é…ç½® |
| `plugins` | `ChatPlugin[]` | âŒ | æ’ä»¶åˆ—è¡¨ |
| `onMessage` | `(message: Message) => void` | âŒ | æ¶ˆæ¯å›è°ƒ |
| `onError` | `(error: Error) => void` | âŒ | é”™è¯¯å›è°ƒ |

#### MemoryConfig

```typescript
interface MemoryConfig {
  maxMessages?: number;      // æœ€å¤§æ¶ˆæ¯æ•° (é»˜è®¤: 50)
  maxTokens?: number;         // æœ€å¤§ Token æ•° (é»˜è®¤: 8000)
  enableSummarization?: boolean;  // å¯ç”¨æ‘˜è¦å‹ç¼©
  systemPromptLocked?: boolean;   // é”å®šç³»ç»Ÿæç¤ºè¯
}
```

### æ’ä»¶ç³»ç»Ÿ

#### åˆ›å»ºæ’ä»¶

```typescript
import { ChatPlugin } from '@kendent/ai-chat-widget';

const searchPlugin: ChatPlugin = {
  name: 'search',
  beforeSend: async (ctx) => {
    // åœ¨å‘é€å‰ä¿®æ”¹æ¶ˆæ¯
    return ctx;
  },
  afterReceive: async (ctx) => {
    // åœ¨æ¥æ”¶åå¤„ç†å“åº”
    return ctx.response;
  },
  onToolCall: async (tool, ctx) => {
    // å¤„ç†å·¥å…·è°ƒç”¨
    return result;
  },
};
```

#### ä½¿ç”¨æ’ä»¶

```tsx
<AIChatWidget
  provider="openai"
  apiKey="sk-xxx"
  model="gpt-4"
  plugins={[searchPlugin]}
/>
```

### é«˜çº§ç”¨æ³•

#### è‡ªå®šä¹‰ Provider

```typescript
import { BaseProvider, ProviderHub } from '@kendent/ai-chat-widget';

class CustomProvider extends BaseProvider {
  name = 'custom';
  
  async *stream(messages, config, signal) {
    // å®ç°æµå¼æ¥å£
  }
  
  async chat(messages, config, signal) {
    // å®ç°èŠå¤©æ¥å£
  }
}

const hub = new ProviderHub();
hub.register(new CustomProvider());
```

#### ç›´æ¥ä½¿ç”¨ Orchestrator

```typescript
import { ConversationOrchestrator } from '@kendent/ai-chat-widget';

const orchestrator = new ConversationOrchestrator({
  provider: {
    provider: 'openai',
    apiKey: 'sk-xxx',
    model: 'gpt-4',
  },
  memory: 50,
  systemPrompt: 'You are a helpful assistant',
});

const response = await orchestrator.sendMessage('Hello!', {
  onChunk: (chunk) => console.log(chunk),
  onComplete: (message) => console.log(message),
});
```

### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UI Layer    â”‚
â”‚ ChatWindow UI â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation  â”‚  â† æ ¸å¿ƒå¼•æ“
â”‚ Orchestrator  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Memory â”‚ â”‚Prompt â”‚ â”‚Providerâ”‚
â”‚Window â”‚ â”‚Engine â”‚ â”‚  Hub  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ”¯æŒçš„æ¨¡å‹

- **OpenAI**: GPT-4, GPT-3.5-turbo, etc.
- **Claude**: Claude 3 Opus, Sonnet, Haiku
- **Moonshot**: Moonshot-v1-8k, Moonshot-v1-32k
- **Local**: Ollama, LM Studio, etc. (OpenAI-compatible)

### è®¸å¯è¯

MIT
